{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some cool title for our paper\n",
      "=============================\n",
      "\n",
      "## Motivations ( or previous work )\n",
      "we are tackeling the problem of segmenting lesions and tissues in BUS from as a metric labeling problem using a discrete optimization approach. Here are some of the characteristics:\n",
      "\n",
      "- Minimization technique : Graph-Cuts (GC)\n",
      "- Data term : supervised Machine Learning (ML)\n",
      "- Pair-wise term: {Markov/Conditional} Random fields (MRF/CRF)\n",
      "\n",
      "### Current method PROS & CONS\n",
      "\n",
      "1. Advantages \n",
      "\n",
      "    1. GC is fast.\n",
      "    2. supervised ML offers a clean and powerful manner of designing the data term.\n",
      "\n",
      "2. Disadvantages\n",
      "\n",
      "    1. Only basic constrains over the final segmentation contour\n",
      "    2. On ML task:\n",
      "    \n",
      "        1. Medical Image Analysis (MIA) is a big data problem (images are big)\n",
      "        2. MIA databases have limited #images and limited Ground Truth (GT)\n",
      "        3. Noisy features / unk. models highly overlapped\n",
      "        4. GT noise, Labeling disagreement between experts\n",
      "        \n",
      "## Random thoughts\n",
      "\n",
      "1. Using ML and Pattern Recognition (PR) to generate reliable stochastic models of the images elements from a reduced dataset (2.B.B) requires robust features and robust training policies. Which is not applicable (2.B.C, 2.B.D)\n",
      "\n",
      "2. New ML approaches like sparse-coding and deep-learning are robust to noise in data, training, etc.. in expenses of more elaborated training techniques usually by minimizing a loss-function. These loss-fuctions can take many shapes and not all of them are appropiated for all the applications. However most of the time loss-function limitations are compensated by larger training sets and policies.\n",
      "\n",
      "3. At the end of the day, when applying this new ML techniques, a metric that splits the data as much as possible is learned. \n",
      "\n",
      "4. Learning this metric from a small pool of noisy data with little knowledge about the features-lablels noise or their relation choosing the appropriated tool is crutial. \n",
      "     1. There's a relation between the features noise and the labeling noise (inconsistancy in experts delineations).\n",
      "     2. Experts produce different delineations based on their criteria (intra-observer variation) but also their image interpretation (features interpretation --> inter-observer variation)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Some Needed Code"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are some details of the classes created in order to facilitate the job afterwards. This section contains some design class diagrams, comments, and code.\n",
      "> cheers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### This is needed in order to generate inline uml diagrames based on plantuml\n",
      "\n",
      "# The plantuml_magics extension is needed. It can be downloaded by uncommenting the following line\n",
      "# %install_ext https://raw.githubusercontent.com/sberke/ipython-plantuml/master/plantuml_magics.py\n",
      "%load_ext plantuml_magics\n",
      "\n",
      "# plantuml should be in the current directory. It can be downloaded in the following link\n",
      "# http://sourceforge.net/projects/plantuml/files/plantuml.jar/download\n",
      "import glob\n",
      "glob.glob(r'./*.jar');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%plantuml classes\n",
      "\n",
      "@startuml\n",
      "Class01 <|-- Class02\n",
      "Class03 *-- Class04\n",
      "Class05 o-- Class06\n",
      "Class07 .. Class08\n",
      "Class09 -- Class10\n",
      "Class11 <|.. Class12onse    \n",
      "@enduml  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This line configures matplotlib to show figures embedded in the notebook, \n",
      "# instead of opening a new window for each figure. More about that later. \n",
      "# If you are using an old version of IPython, try using '%pylab inline' instead.\n",
      "%matplotlib inline\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.mlab as mlab\n",
      "from sklearn.decomposition import PCA\n",
      "import math"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Proposal \n",
      "\n",
      "1. Generate synthetic datasets from real data by sampling real data according to some criteria. Trying to simulate a progressive dificultie in the data.\n",
      "\n",
      "    I think that what we are proposing has a relation with the monte-carlo-metropolis experiment. Plus varying the conditions to increase dificultie for each dataset.\n",
      "    1. The inital data $D$ is in the hyperspace $R^N$ and each data point belongs to a single class $c \\in C$ and is labeled as label $l \\in L$ associated. Such that $\\exists ! d | \\<d,l_1\\> & \\<d,l_1\\> & l_1 \\ne l_2$.\n",
      "    2. The metric is wanted to do something (representing the data) while keeping the data separated. Therefore, the metric is inteded to split the data on a lower dimensional space since the relation of feature dimension and data sparcity is quadratic. The ideal case is keep the data separated as much as possible with as little dimensionality as possible. \n",
      "    3. A linear mapping $f \\in \\{F \\cap linear}$ where $F:R^N \\to R^m$ is done. $f$ is intended such that everything still is a mess and the MAP of a point in the lower dimension mean nothing.\n",
      "    4. This lower dimension space is used to sample a dataset $S$ from the initial data $D$ such that $S$ is a subset of $D$, $S \\subset D$. In order to determine $S$ the classes are sampled in the lower dimension space. This sampling could be methodical, monte-carlo or any other one but it is chosen to be a gaussian sampling centered on the mode of the class in the lower space. Let $x$ be a coordinate in the reduced space $x \\in R^m$. Then the probility of chosing a sample $d$ from class $c$ such that $x=f(d)$ in order to represent this class $c$ for the subset $S$ can be expresed as $P(x,c) \\sim N( mode(f(d)), \\sigma)$. \n",
      "    5. Hipothesis: \n",
      "    Using the samples in $S$ and mapping them to $R^m$, The separhability of the data should be AT WORST, as good as the separhability between the classes sampling strategy.\n",
      "\n",
      "2. For each dataset try to split the data using different methodologies and see which is more constant. \n",
      "\n",
      "    1. By pushing this WORST CASE further and further meaning increasing sigma so that the sampling strategy gets closer and closer to the methodical or monte-carlo sampling strategy and $x$ become equiproval for all classes only metrics properly encoding the noise of the data and the labels should be able to keep spliting the samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}